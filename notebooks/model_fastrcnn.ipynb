{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchVision Object Detection\n",
    "\n",
    "Adapted by Laurentius von Liechti from this tutorial: \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from utils.engine import train_one_epoch, evaluate\n",
    "import utils.utils as utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "Fetch these into the utils folder if they aren't already present.\n",
    "\n",
    "* os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "* os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "* os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "* os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "* os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = \"../data/train/images/\"\n",
    "train_label_dir = \"../data/train/labels/\"\n",
    "val_image_dir = \"../data/valid/images/\"\n",
    "val_label_dir = \"../data/valid/labels/\"\n",
    "\n",
    "label_array = ['Ambulance', 'Bus', 'Car', 'Motorcycle', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Group3DataSet class for loading images and labels\n",
    "\n",
    "__init__ - initializes the class with image and label directories\n",
    "__len__ - returns the number of images\n",
    "__getitem__ - returns the image and label at the given index\n",
    "get_pic - returns the image at the given path\n",
    "get_target - returns the target at the given path\n",
    "load - loads the images and labels from the directories\n",
    "\n",
    "Parameters:\n",
    "    image_dir - the directory containing the images\n",
    "    label_dir - the directory containing the labels\n",
    "\"\"\"\n",
    "class Group3Dataset(Dataset):\n",
    "    IMG_SIZE = 416\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes the class with image and label directories\n",
    "\n",
    "    Parameters:\n",
    "        image_dir - the directory containing the images\n",
    "        label_dir - the directory containing the labels\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.load(image_dir, label_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        target = self.targets[idx]\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the image at the given path\n",
    "\n",
    "    Parameters:\n",
    "        img_path - the path to the image\n",
    "\n",
    "    Returns:\n",
    "        dimg - the image tensor\n",
    "    \"\"\"\n",
    "    def get_pic(self, img_path):\n",
    "        img = read_image(img_path)\n",
    "        tvimg = tv_tensors.Image(img)\n",
    "        dimg = torchvision.transforms.v2.functional.to_dtype(tvimg, torch.float32, scale=True)\n",
    "        return dimg\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the target at the given label_path for the given img_path\n",
    "\n",
    "    Converts the [x/y]center/width/height label to a tensor and \n",
    "    calculates the bounding box coordinates for each label in the label file\n",
    "\n",
    "    It returns a dictionary containing the boxes, labels, image_id, area, \n",
    "    and iscrowd expected by the FasterRCNN model\n",
    "\n",
    "    Parameters:\n",
    "        img_path - the path to the image\n",
    "        label_path - the path to the label\n",
    "\n",
    "    Returns:\n",
    "        dict - a target dictionary\n",
    "    \"\"\"\n",
    "    def get_target(self, img_path, label_path):\n",
    "        rows = open(label_path).read().strip().split(\"\\n\")\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        for row in rows:\n",
    "            label, x_center, y_center, width, height = re.split(r'\\s+', row)\n",
    "            x_start = int((float(x_center) - float(width)/2)*self.IMG_SIZE)\n",
    "            y_start = int((float(y_center) - float(height)/2)*self.IMG_SIZE)\n",
    "            x_end = int((float(x_center) + float(width)/2)*self.IMG_SIZE)\n",
    "            y_end = int((float(y_center) + float(height)/2)*self.IMG_SIZE)\n",
    "            boxes.append([x_start, y_start, x_end, y_end])\n",
    "            labels.append(int(label) + 1) # 0 is reserved for background\n",
    "            areas.append((x_end - x_start) * (y_end - y_start))\n",
    "        return {\n",
    "            'boxes': tv_tensors.BoundingBoxes(\n",
    "                boxes,\n",
    "                format=tv_tensors.BoundingBoxFormat.XYXY, \n",
    "                canvas_size=(self.IMG_SIZE, self.IMG_SIZE)), \n",
    "            'labels': torch.tensor(labels, dtype=torch.int64), \n",
    "            'image_id': img_path,\n",
    "            'area': torch.Tensor(areas),\n",
    "            'iscrowd': torch.zeros((len(labels),), dtype=torch.int64)\n",
    "            }\n",
    "\n",
    "    def load(self, image_dir, label_dir):\n",
    "        image_files = os.listdir(image_dir)\n",
    "        label_files = os.listdir(label_dir)\n",
    "        image_files.sort()\n",
    "        label_files.sort()\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for i, img_file in enumerate(image_files):\n",
    "            image_path = image_dir + img_file\n",
    "            label_path = label_dir + label_files[i]\n",
    "            self.images.append(self.get_pic(image_path))\n",
    "            self.targets.append(self.get_target(image_path, label_path))\n",
    "            \n",
    "        return self.images, self.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the custom dataset\n",
    "\n",
    "* Use a small batch size and simple model to test our custom dataset to ensure it has all the required data structures in the right format for training and testing, end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.4265, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1069, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0052, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0092, grad_fn=<DivBackward0>)}\n",
      "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "train_ds = Group3Dataset(train_image_dir, train_label_dir, get_transform(train=True))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Epoch: [0]  [  0/439]  eta: 0:17:43  lr: 0.000016  loss: 2.2320 (2.2320)  loss_classifier: 1.9939 (1.9939)  loss_box_reg: 0.1977 (0.1977)  loss_objectness: 0.0387 (0.0387)  loss_rpn_box_reg: 0.0017 (0.0017)  time: 2.4221  data: 0.0003\n",
      "Epoch: [0]  [ 10/439]  eta: 0:16:32  lr: 0.000130  loss: 2.0842 (2.0559)  loss_classifier: 1.8430 (1.7580)  loss_box_reg: 0.1977 (0.2584)  loss_objectness: 0.0236 (0.0303)  loss_rpn_box_reg: 0.0036 (0.0091)  time: 2.3131  data: 0.0005\n",
      "Epoch: [0]  [ 20/439]  eta: 0:15:59  lr: 0.000244  loss: 1.6199 (1.5703)  loss_classifier: 1.2451 (1.2578)  loss_box_reg: 0.2048 (0.2766)  loss_objectness: 0.0200 (0.0281)  loss_rpn_box_reg: 0.0036 (0.0079)  time: 2.2830  data: 0.0005\n",
      "Epoch: [0]  [ 30/439]  eta: 0:15:34  lr: 0.000359  loss: 0.6374 (1.2241)  loss_classifier: 0.3174 (0.9328)  loss_box_reg: 0.2276 (0.2588)  loss_objectness: 0.0194 (0.0255)  loss_rpn_box_reg: 0.0054 (0.0069)  time: 2.2711  data: 0.0005\n",
      "Epoch: [0]  [ 40/439]  eta: 0:15:12  lr: 0.000473  loss: 0.4609 (1.0409)  loss_classifier: 0.2490 (0.7668)  loss_box_reg: 0.1933 (0.2459)  loss_objectness: 0.0095 (0.0212)  loss_rpn_box_reg: 0.0054 (0.0070)  time: 2.2855  data: 0.0005\n",
      "Epoch: [0]  [ 50/439]  eta: 0:14:51  lr: 0.000587  loss: 0.4571 (0.9482)  loss_classifier: 0.2377 (0.6721)  loss_box_reg: 0.1985 (0.2498)  loss_objectness: 0.0085 (0.0195)  loss_rpn_box_reg: 0.0060 (0.0069)  time: 2.3008  data: 0.0005\n",
      "Epoch: [0]  [ 60/439]  eta: 0:14:28  lr: 0.000701  loss: 0.5048 (0.8926)  loss_classifier: 0.2147 (0.6059)  loss_box_reg: 0.2609 (0.2611)  loss_objectness: 0.0092 (0.0188)  loss_rpn_box_reg: 0.0054 (0.0067)  time: 2.2991  data: 0.0005\n",
      "Epoch: [0]  [ 70/439]  eta: 0:14:05  lr: 0.000815  loss: 0.4400 (0.8345)  loss_classifier: 0.2147 (0.5487)  loss_box_reg: 0.2354 (0.2581)  loss_objectness: 0.0090 (0.0205)  loss_rpn_box_reg: 0.0041 (0.0072)  time: 2.2873  data: 0.0004\n",
      "Epoch: [0]  [ 80/439]  eta: 0:13:41  lr: 0.000929  loss: 0.4311 (0.7985)  loss_classifier: 0.1749 (0.5071)  loss_box_reg: 0.2118 (0.2613)  loss_objectness: 0.0109 (0.0223)  loss_rpn_box_reg: 0.0037 (0.0078)  time: 2.2801  data: 0.0004\n",
      "Epoch: [0]  [ 90/439]  eta: 0:13:19  lr: 0.001043  loss: 0.4184 (0.7690)  loss_classifier: 0.1749 (0.4762)  loss_box_reg: 0.2116 (0.2633)  loss_objectness: 0.0109 (0.0221)  loss_rpn_box_reg: 0.0028 (0.0074)  time: 2.2922  data: 0.0005\n",
      "Epoch: [0]  [100/439]  eta: 0:12:56  lr: 0.001157  loss: 0.4184 (0.7341)  loss_classifier: 0.1449 (0.4453)  loss_box_reg: 0.2264 (0.2604)  loss_objectness: 0.0073 (0.0213)  loss_rpn_box_reg: 0.0030 (0.0072)  time: 2.3027  data: 0.0005\n",
      "Epoch: [0]  [110/439]  eta: 0:12:34  lr: 0.001271  loss: 0.4408 (0.7140)  loss_classifier: 0.1860 (0.4233)  loss_box_reg: 0.2548 (0.2626)  loss_objectness: 0.0079 (0.0208)  loss_rpn_box_reg: 0.0062 (0.0073)  time: 2.3113  data: 0.0004\n",
      "Epoch: [0]  [120/439]  eta: 0:12:12  lr: 0.001385  loss: 0.3598 (0.6875)  loss_classifier: 0.1362 (0.4025)  loss_box_reg: 0.2181 (0.2577)  loss_objectness: 0.0132 (0.0203)  loss_rpn_box_reg: 0.0050 (0.0070)  time: 2.3145  data: 0.0003\n",
      "Epoch: [0]  [130/439]  eta: 0:11:48  lr: 0.001499  loss: 0.2389 (0.6591)  loss_classifier: 0.1028 (0.3822)  loss_box_reg: 0.1312 (0.2507)  loss_objectness: 0.0088 (0.0194)  loss_rpn_box_reg: 0.0038 (0.0069)  time: 2.2871  data: 0.0003\n",
      "Epoch: [0]  [140/439]  eta: 0:11:26  lr: 0.001613  loss: 0.3259 (0.6460)  loss_classifier: 0.1231 (0.3685)  loss_box_reg: 0.1464 (0.2478)  loss_objectness: 0.0060 (0.0223)  loss_rpn_box_reg: 0.0059 (0.0073)  time: 2.2908  data: 0.0004\n",
      "Epoch: [0]  [150/439]  eta: 0:11:03  lr: 0.001727  loss: 0.3851 (0.6291)  loss_classifier: 0.1527 (0.3540)  loss_box_reg: 0.1906 (0.2435)  loss_objectness: 0.0125 (0.0237)  loss_rpn_box_reg: 0.0093 (0.0079)  time: 2.3202  data: 0.0005\n",
      "Epoch: [0]  [160/439]  eta: 0:10:41  lr: 0.001841  loss: 0.3604 (0.6114)  loss_classifier: 0.1412 (0.3411)  loss_box_reg: 0.1906 (0.2395)  loss_objectness: 0.0127 (0.0230)  loss_rpn_box_reg: 0.0053 (0.0078)  time: 2.3229  data: 0.0005\n",
      "Epoch: [0]  [170/439]  eta: 0:10:18  lr: 0.001955  loss: 0.3225 (0.5978)  loss_classifier: 0.1233 (0.3298)  loss_box_reg: 0.1902 (0.2373)  loss_objectness: 0.0146 (0.0230)  loss_rpn_box_reg: 0.0047 (0.0078)  time: 2.3190  data: 0.0004\n",
      "Epoch: [0]  [180/439]  eta: 0:09:56  lr: 0.002069  loss: 0.3351 (0.5826)  loss_classifier: 0.1248 (0.3187)  loss_box_reg: 0.1864 (0.2335)  loss_objectness: 0.0173 (0.0228)  loss_rpn_box_reg: 0.0037 (0.0076)  time: 2.3264  data: 0.0005\n",
      "Epoch: [0]  [190/439]  eta: 0:09:33  lr: 0.002183  loss: 0.3071 (0.5682)  loss_classifier: 0.1248 (0.3082)  loss_box_reg: 0.1260 (0.2300)  loss_objectness: 0.0104 (0.0221)  loss_rpn_box_reg: 0.0034 (0.0079)  time: 2.3315  data: 0.0005\n",
      "Epoch: [0]  [200/439]  eta: 0:09:10  lr: 0.002297  loss: 0.2319 (0.5555)  loss_classifier: 0.1000 (0.2989)  loss_box_reg: 0.1071 (0.2271)  loss_objectness: 0.0030 (0.0216)  loss_rpn_box_reg: 0.0027 (0.0079)  time: 2.3241  data: 0.0004\n",
      "Epoch: [0]  [210/439]  eta: 0:08:47  lr: 0.002411  loss: 0.2319 (0.5478)  loss_classifier: 0.1000 (0.2924)  loss_box_reg: 0.1304 (0.2261)  loss_objectness: 0.0030 (0.0211)  loss_rpn_box_reg: 0.0028 (0.0082)  time: 2.3100  data: 0.0004\n",
      "Epoch: [0]  [220/439]  eta: 0:08:24  lr: 0.002525  loss: 0.2636 (0.5357)  loss_classifier: 0.1300 (0.2851)  loss_box_reg: 0.1239 (0.2220)  loss_objectness: 0.0050 (0.0205)  loss_rpn_box_reg: 0.0036 (0.0081)  time: 2.3040  data: 0.0005\n",
      "Epoch: [0]  [230/439]  eta: 0:08:01  lr: 0.002639  loss: 0.2797 (0.5278)  loss_classifier: 0.1404 (0.2791)  loss_box_reg: 0.1320 (0.2204)  loss_objectness: 0.0075 (0.0202)  loss_rpn_box_reg: 0.0036 (0.0080)  time: 2.3080  data: 0.0005\n",
      "Epoch: [0]  [240/439]  eta: 0:07:38  lr: 0.002753  loss: 0.2211 (0.5185)  loss_classifier: 0.1045 (0.2726)  loss_box_reg: 0.1312 (0.2180)  loss_objectness: 0.0100 (0.0200)  loss_rpn_box_reg: 0.0027 (0.0079)  time: 2.3031  data: 0.0005\n",
      "Epoch: [0]  [250/439]  eta: 0:07:15  lr: 0.002867  loss: 0.1995 (0.5102)  loss_classifier: 0.0955 (0.2672)  loss_box_reg: 0.0908 (0.2151)  loss_objectness: 0.0030 (0.0199)  loss_rpn_box_reg: 0.0033 (0.0079)  time: 2.3005  data: 0.0004\n",
      "Epoch: [0]  [260/439]  eta: 0:06:52  lr: 0.002981  loss: 0.2586 (0.5042)  loss_classifier: 0.1391 (0.2634)  loss_box_reg: 0.1152 (0.2134)  loss_objectness: 0.0043 (0.0196)  loss_rpn_box_reg: 0.0061 (0.0078)  time: 2.3140  data: 0.0004\n",
      "Epoch: [0]  [270/439]  eta: 0:06:29  lr: 0.003096  loss: 0.2586 (0.4961)  loss_classifier: 0.1391 (0.2582)  loss_box_reg: 0.1128 (0.2103)  loss_objectness: 0.0100 (0.0197)  loss_rpn_box_reg: 0.0056 (0.0080)  time: 2.3109  data: 0.0004\n",
      "Epoch: [0]  [280/439]  eta: 0:06:06  lr: 0.003210  loss: 0.2412 (0.4892)  loss_classifier: 0.1231 (0.2539)  loss_box_reg: 0.0982 (0.2078)  loss_objectness: 0.0136 (0.0195)  loss_rpn_box_reg: 0.0052 (0.0080)  time: 2.3109  data: 0.0005\n",
      "Epoch: [0]  [290/439]  eta: 0:05:43  lr: 0.003324  loss: 0.2040 (0.4806)  loss_classifier: 0.1049 (0.2489)  loss_box_reg: 0.0931 (0.2044)  loss_objectness: 0.0069 (0.0194)  loss_rpn_box_reg: 0.0049 (0.0079)  time: 2.3242  data: 0.0005\n",
      "Epoch: [0]  [300/439]  eta: 0:05:20  lr: 0.003438  loss: 0.2538 (0.4746)  loss_classifier: 0.1049 (0.2449)  loss_box_reg: 0.1080 (0.2020)  loss_objectness: 0.0090 (0.0195)  loss_rpn_box_reg: 0.0060 (0.0082)  time: 2.3163  data: 0.0004\n",
      "Epoch: [0]  [310/439]  eta: 0:04:57  lr: 0.003552  loss: 0.1723 (0.4648)  loss_classifier: 0.0792 (0.2395)  loss_box_reg: 0.0800 (0.1980)  loss_objectness: 0.0067 (0.0193)  loss_rpn_box_reg: 0.0051 (0.0080)  time: 2.3150  data: 0.0004\n",
      "Epoch: [0]  [320/439]  eta: 0:04:34  lr: 0.003666  loss: 0.1635 (0.4585)  loss_classifier: 0.0792 (0.2353)  loss_box_reg: 0.0769 (0.1961)  loss_objectness: 0.0048 (0.0191)  loss_rpn_box_reg: 0.0030 (0.0080)  time: 2.3151  data: 0.0005\n",
      "Epoch: [0]  [330/439]  eta: 0:04:11  lr: 0.003780  loss: 0.2572 (0.4564)  loss_classifier: 0.1176 (0.2326)  loss_box_reg: 0.1477 (0.1965)  loss_objectness: 0.0083 (0.0193)  loss_rpn_box_reg: 0.0053 (0.0080)  time: 2.3055  data: 0.0004\n",
      "Epoch: [0]  [340/439]  eta: 0:03:48  lr: 0.003894  loss: 0.2901 (0.4521)  loss_classifier: 0.1142 (0.2289)  loss_box_reg: 0.1607 (0.1959)  loss_objectness: 0.0094 (0.0191)  loss_rpn_box_reg: 0.0074 (0.0082)  time: 2.3371  data: 0.0004\n",
      "Epoch: [0]  [350/439]  eta: 0:03:25  lr: 0.004008  loss: 0.2396 (0.4460)  loss_classifier: 0.0807 (0.2255)  loss_box_reg: 0.1281 (0.1936)  loss_objectness: 0.0069 (0.0187)  loss_rpn_box_reg: 0.0051 (0.0082)  time: 2.3410  data: 0.0005\n",
      "Epoch: [0]  [360/439]  eta: 0:03:02  lr: 0.004122  loss: 0.2301 (0.4436)  loss_classifier: 0.1151 (0.2236)  loss_box_reg: 0.1068 (0.1922)  loss_objectness: 0.0080 (0.0195)  loss_rpn_box_reg: 0.0042 (0.0083)  time: 2.3184  data: 0.0005\n",
      "Epoch: [0]  [370/439]  eta: 0:02:39  lr: 0.004236  loss: 0.2365 (0.4384)  loss_classifier: 0.1037 (0.2205)  loss_box_reg: 0.1068 (0.1904)  loss_objectness: 0.0106 (0.0193)  loss_rpn_box_reg: 0.0046 (0.0082)  time: 2.3160  data: 0.0004\n",
      "Epoch: [0]  [380/439]  eta: 0:02:16  lr: 0.004350  loss: 0.1786 (0.4335)  loss_classifier: 0.0813 (0.2175)  loss_box_reg: 0.0925 (0.1886)  loss_objectness: 0.0075 (0.0191)  loss_rpn_box_reg: 0.0041 (0.0082)  time: 2.3134  data: 0.0004\n",
      "Epoch: [0]  [390/439]  eta: 0:01:53  lr: 0.004464  loss: 0.2345 (0.4291)  loss_classifier: 0.0851 (0.2147)  loss_box_reg: 0.1133 (0.1872)  loss_objectness: 0.0077 (0.0189)  loss_rpn_box_reg: 0.0043 (0.0083)  time: 2.3198  data: 0.0004\n",
      "Epoch: [0]  [400/439]  eta: 0:01:30  lr: 0.004578  loss: 0.2208 (0.4239)  loss_classifier: 0.0851 (0.2119)  loss_box_reg: 0.0970 (0.1850)  loss_objectness: 0.0090 (0.0187)  loss_rpn_box_reg: 0.0048 (0.0082)  time: 2.3118  data: 0.0004\n",
      "Epoch: [0]  [410/439]  eta: 0:01:06  lr: 0.004692  loss: 0.2059 (0.4204)  loss_classifier: 0.0783 (0.2099)  loss_box_reg: 0.0911 (0.1836)  loss_objectness: 0.0067 (0.0187)  loss_rpn_box_reg: 0.0044 (0.0082)  time: 2.3121  data: 0.0005\n",
      "Epoch: [0]  [420/439]  eta: 0:00:43  lr: 0.004806  loss: 0.2001 (0.4152)  loss_classifier: 0.0903 (0.2072)  loss_box_reg: 0.0847 (0.1815)  loss_objectness: 0.0063 (0.0185)  loss_rpn_box_reg: 0.0031 (0.0081)  time: 2.3152  data: 0.0005\n",
      "Epoch: [0]  [430/439]  eta: 0:00:20  lr: 0.004920  loss: 0.1767 (0.4108)  loss_classifier: 0.0788 (0.2046)  loss_box_reg: 0.0755 (0.1799)  loss_objectness: 0.0061 (0.0182)  loss_rpn_box_reg: 0.0043 (0.0080)  time: 2.3021  data: 0.0005\n",
      "Epoch: [0]  [438/439]  eta: 0:00:02  lr: 0.005000  loss: 0.1943 (0.4096)  loss_classifier: 0.0800 (0.2034)  loss_box_reg: 0.0939 (0.1799)  loss_objectness: 0.0061 (0.0182)  loss_rpn_box_reg: 0.0055 (0.0081)  time: 2.2991  data: 0.0004\n",
      "Epoch: [0] Total time: 0:16:53 (2.3087 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/250]  eta: 0:02:36  model_time: 0.6222 (0.6222)  evaluator_time: 0.0027 (0.0027)  time: 0.6250  data: 0.0001\n",
      "Test:  [100/250]  eta: 0:01:29  model_time: 0.5867 (0.5966)  evaluator_time: 0.0005 (0.0007)  time: 0.5851  data: 0.0001\n",
      "Test:  [200/250]  eta: 0:00:29  model_time: 0.5774 (0.5894)  evaluator_time: 0.0006 (0.0007)  time: 0.5790  data: 0.0001\n",
      "Test:  [249/250]  eta: 0:00:00  model_time: 0.5811 (0.5876)  evaluator_time: 0.0006 (0.0007)  time: 0.5839  data: 0.0001\n",
      "Test: Total time: 0:02:27 (0.5884 s / it)\n",
      "Averaged stats: model_time: 0.5811 (0.5876)  evaluator_time: 0.0006 (0.0007)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.551\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.370\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.553\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.145\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\n"
     ]
    }
   ],
   "source": [
    "# Leaving out MPS because of the float32 limitation\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 6\n",
    "# use our dataset and defined transformations\n",
    "dataset = Group3Dataset(train_image_dir, train_label_dir, get_transform(train=True))\n",
    "dataset_test = Group3Dataset(val_image_dir, val_label_dir, get_transform(train=False))\n",
    "\n",
    "# Skip this because we already have a train/test split for consistent model comparison\n",
    "# split the dataset in train and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = len(label_array) + 1  # # classes + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 1\n",
    "\n",
    "evaluators = []\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    eval = evaluate(model, data_loader_test, device=device)\n",
    "    evaluators.append(eval)\n",
    "\n",
    "torch.save(model.state_dict(), \"model_factrcnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/250]  eta: 0:02:37  model_time: 0.6249 (0.6249)  evaluator_time: 0.0026 (0.0026)  time: 0.6281  data: 0.0005\n",
      "Test:  [100/250]  eta: 0:01:29  model_time: 0.5936 (0.5949)  evaluator_time: 0.0005 (0.0007)  time: 0.5953  data: 0.0001\n",
      "Test:  [200/250]  eta: 0:00:29  model_time: 0.5812 (0.5946)  evaluator_time: 0.0006 (0.0007)  time: 0.5837  data: 0.0001\n",
      "Test:  [249/250]  eta: 0:00:00  model_time: 0.5833 (0.5935)  evaluator_time: 0.0006 (0.0007)  time: 0.5845  data: 0.0001\n",
      "Test: Total time: 0:02:28 (0.5943 s / it)\n",
      "Averaged stats: model_time: 0.5833 (0.5935)  evaluator_time: 0.0006 (0.0007)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.551\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.370\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.553\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.145\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\n"
     ]
    }
   ],
   "source": [
    "eval = evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "class 0 AP : 0.6691170078730871\n",
      "class 1 AP : 0.6807712099776464\n",
      "class 2 AP : 0.39838980144066183\n",
      "class 3 AP : 0.7100248955312386\n",
      "class 4 AP : 0.2971397381800174\n",
      "(all) mAP : 0.5510885306005303\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.551\n",
      "[0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "class 0 AP : 0.47462342091759946\n",
      "class 1 AP : 0.35715840629281875\n",
      "class 2 AP : 0.20780115112818254\n",
      "class 3 AP : 0.4084303018104592\n",
      "class 4 AP : 0.19355458969180267\n",
      "(all) mAP : 0.3283135739681725\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      "[0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "class 0 AR : 0.70625\n",
      "class 1 AR : 0.5478260869565217\n",
      "class 2 AR : 0.5042016806722689\n",
      "class 3 AR : 0.6000000000000001\n",
      "class 4 AR : 0.4066666666666666\n",
      "(all) mAR : 0.5529888868590915\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ap</th>\n",
       "      <th>ap50</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ambulance</td>\n",
       "      <td>0.474623</td>\n",
       "      <td>0.669117</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.474623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bus</td>\n",
       "      <td>0.357158</td>\n",
       "      <td>0.680771</td>\n",
       "      <td>0.547826</td>\n",
       "      <td>0.357158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "      <td>0.207801</td>\n",
       "      <td>0.398390</td>\n",
       "      <td>0.504202</td>\n",
       "      <td>0.207801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>0.408430</td>\n",
       "      <td>0.710025</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.408430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Truck</td>\n",
       "      <td>0.193555</td>\n",
       "      <td>0.297140</td>\n",
       "      <td>0.406667</td>\n",
       "      <td>0.193555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all</td>\n",
       "      <td>0.328314</td>\n",
       "      <td>0.551089</td>\n",
       "      <td>0.552989</td>\n",
       "      <td>0.328314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class        ap      ap50         r         p\n",
       "0   Ambulance  0.474623  0.669117  0.706250  0.474623\n",
       "1         Bus  0.357158  0.680771  0.547826  0.357158\n",
       "2         Car  0.207801  0.398390  0.504202  0.207801\n",
       "3  Motorcycle  0.408430  0.710025  0.600000  0.408430\n",
       "4       Truck  0.193555  0.297140  0.406667  0.193555\n",
       "5         all  0.328314  0.551089  0.552989  0.328314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiclass_summarize( ce, ap=1, iouThr=None, areaRng='all', maxDets=100 ):\n",
    "    p = ce.params\n",
    "    print(p.iouThrs)\n",
    "    iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "    typeStr = '(AP)' if ap==1 else '(AR)'\n",
    "    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "        if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "    aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "    mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "    if ap == 1:\n",
    "        # dimension of precision: [TxRxKxAxM]\n",
    "        s = ce.eval['precision']\n",
    "        # IoU\n",
    "        if iouThr is not None:\n",
    "            t = np.where(iouThr == p.iouThrs)[0]\n",
    "            s = s[t]\n",
    "        s = s[:,:,:,aind,mind]\n",
    "    else:\n",
    "        # dimension of recall: [TxKxAxM]\n",
    "        s = ce.eval['recall']\n",
    "        if iouThr is not None:\n",
    "            t = np.where(iouThr == p.iouThrs)[0]\n",
    "            s = s[t]\n",
    "        s = s[:,:,aind,mind]\n",
    "    metrics = [-1]\n",
    "    if len(s[s>-1]) != 0:\n",
    "        mean_s = np.mean(s[s>-1])\n",
    "        num_classes = len(p.catIds)\n",
    "        metrics = np.zeros((num_classes+1,))\n",
    "        avg = 0.0\n",
    "        label = 'AP' if ap == 1 else 'AR'\n",
    "        for i in range(0, num_classes):\n",
    "            mu = np.mean(s[:,:,i,:]) if ap == 1 else np.mean(s[:,i,:])\n",
    "            print('class {0} {1} : {2}'.format(i,label,mu))\n",
    "            metrics[i] = mu\n",
    "            avg +=mu\n",
    "        mu = avg / num_classes\n",
    "        metrics[num_classes] = mu\n",
    "        print('(all) m{0} : {1}'.format(label, mu))\n",
    "    print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "    return metrics\n",
    "\n",
    "iou_type = \"bbox\"\n",
    "ce = eval.coco_eval[iou_type]\n",
    "ap50 = multiclass_summarize(ce, ap=1, iouThr=0.5) # Print AP for each class\n",
    "ap = multiclass_summarize(ce) # Print AP for each class\n",
    "ar = multiclass_summarize(ce, ap=2) # Print AR for each class\n",
    "\n",
    "# summarize the results\n",
    "import pandas as pd\n",
    "dfMetrics = pd.DataFrame({\n",
    "    \"class\": label_array + ['all'],\n",
    "    \"ap\": ap,\n",
    "    \"ap50\": ap50,\n",
    "    \"r\": ar,\n",
    "    \"p\": ap,\n",
    "})\n",
    "display(dfMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file_name = \"../results/model_fastrcnn_metrics.pkl\"\n",
    "dfMetrics.to_pickle(metrics_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
